{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import math \n",
    "import time\n",
    "import graph\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import array as arr\n",
    "import copy\n",
    "\n",
    "#filepathTXT = \"../edgelists/BlogCatalog-edgelist.txt\"\n",
    "filepathCSV = \"input/BlogCatalog-edgelist.csv\"\n",
    "#embeddingsrecursive = \"../embeddings/BlogCatalog-edgelist.txt.embeddings-recursive\"\n",
    "#embeddingsiterative = \"../embeddings/BlogCatalog-edgelist.txt.embeddings-iterative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseEdgeList2(graph_file, direction = \"undirected\"):\n",
    "    # Create Graph\n",
    "    G = nx.Graph()\n",
    "    # Create head\n",
    "    colNames=[\"Start\", \"End\"]\n",
    "    edgeData = pd.read_csv(filepathCSV, names=colNames)\n",
    "\n",
    "    #Add nodes\n",
    "    nodes = []\n",
    "    #loop throug data records\n",
    "    for i in range (0, edgeData.shape[0]):\n",
    "        #append every node\n",
    "        nodes.append(edgeData.iloc[i,0])\n",
    "        nodes.append(edgeData.iloc[i,1])\n",
    "    #creating a set of nodes    \n",
    "    nodes = set(nodes)\n",
    "    #sorting the nodes in increasing order\n",
    "    uniqueNodes = (list(nodes))\n",
    "    uniqueNodes.sort()\n",
    "    #adding the nodes to the graph\n",
    "    G.add_nodes_from(uniqueNodes)\n",
    "\n",
    "    # Add edges\n",
    "    #loop from 0 to amount of records\n",
    "    edgeCount = 0\n",
    "    for i in range (0, edgeData.shape[0]):\n",
    "        edgeCount += 1\n",
    "        #add the edge to the graph\n",
    "        G.add_edge(edgeData.iloc[i,0], edgeData.iloc[i,1])\n",
    "    print(\"Nodes: \", G.number_of_nodes(),\" Edges: \", G.number_of_edges(), \" loaded from \", graph_file)\n",
    "\n",
    "    if(direction == \"undirected\"):\n",
    "        return G.to_undirected()\n",
    "    else:\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdjNPList(graph):\n",
    "    adjdict = {}\n",
    "    for vertex in graph:\n",
    "        adjdict[vertex] = np.array([n for n in G.neighbors(vertex)]) \n",
    "        np.random.shuffle(adjdict[vertex])\n",
    "    return adjdict\n",
    "\n",
    "def getNodeContextSets(graph):\n",
    "    all_sets = {}\n",
    "    for node in graph:\n",
    "        all_sets[node] = []\n",
    "    return all_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountContextPairs(contextPairs):\n",
    "    countSum = 0\n",
    "    for key, value in contextPairs.items():\n",
    "        countSum += value\n",
    "    print(\"Total value sums up to: \", countSum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#femb_recursive = open(embeddingsrecursive, 'w')\n",
    "#femb_iterative = open(embeddingsiterative, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getToyGraph():\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from([\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\"])\n",
    "    inputfile = \"input/karate.adjlist\"\n",
    "    lines = open(inputfile, \"r\")\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        for node in line[1:]:\n",
    "            G.add_edge(line[0], node);\n",
    "    print(\"Nodes: \", G.number_of_nodes(),\" Edges: \", G.number_of_edges())\n",
    "    #print(\"G.nodes ->\", G.nodes)\n",
    "    # Draw graph\n",
    "    #nx.draw(G, with_labels = True)\n",
    "    #plt.show()\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:  10312  Edges:  333983  loaded from  input/BlogCatalog-edgelist.csv\n"
     ]
    }
   ],
   "source": [
    "# Set the actual parameters and graph\n",
    "WALK_LENGHT = 40\n",
    "BUDGET = 80\n",
    "ORIGINAL_WINDOW_SIZE = 10\n",
    "\n",
    "EPSILON = 0.5\n",
    "WINDOW_SIZE = ORIGINAL_WINDOW_SIZE*2+1\n",
    "QUEUE_BUFFER_SIZE = (BUDGET*WALK_LENGHT)-(BUDGET*2) + 1\n",
    "#G = parseEdgeList1(filepathTXT) #String labels \n",
    "\n",
    "G = parseEdgeList2(filepathCSV) # Integer labels\n",
    "adjdict = getAdjNPList(G)\n",
    "all_sets = getNodeContextSets(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set toy parameters and graph\n",
    "WALK_LENGHT = 5\n",
    "BUDGET = 2\n",
    "ORIGINAL_WINDOW_SIZE = 1\n",
    "\n",
    "EPSILON = 0.5\n",
    "WINDOW_SIZE = ORIGINAL_WINDOW_SIZE*2+1\n",
    "QUEUE_BUFFER_SIZE = (BUDGET*WALK_LENGHT)-(BUDGET*2) + 1\n",
    "\n",
    "G = getToyGraph()\n",
    "adjdict = getAdjNPList(G)\n",
    "all_sets = getNodeContextSets(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSetCount(startvertex, context_pair, context_budget):\n",
    "    for i in range(0, context_budget):\n",
    "        all_sets[startvertex].append(context_pair)\n",
    "\n",
    "def chooseNodes(list_nodes, sample_size):\n",
    "    random.seed(0)\n",
    "    return random.sample(population=list_nodes, k=sample_size) # 2.22 s Â± 179 ms per loop  (labesl str)\n",
    "\n",
    "def getPerNodeBudget(numNodes, budget):\n",
    "    return math.floor(budget/numNodes)\n",
    "\n",
    "def storeContextPairs(context_pair, budget, context_pairs):\n",
    "    if context_pair not in context_pairs:\n",
    "        context_pairs[context_pair] = budget\n",
    "    else:\n",
    "        context_pairs[context_pair] = context_pairs[context_pair] + budget\n",
    "        \n",
    "def updateContextPairs(startvertex, window, window_count, context_pairs):\n",
    "    lastNode = window[window_count]\n",
    "    labelOfLastNode, budgetOfLastNode = lastNode\n",
    "    index_count = 0\n",
    "    window_except_last = window[:window_count]\n",
    "    window_reversed = window_except_last[::-1]\n",
    "    for node in window_reversed:\n",
    "        node_label = node[0]\n",
    "        if index_count == ORIGINAL_WINDOW_SIZE:\n",
    "            break\n",
    "        context_pair1 = (str(labelOfLastNode) +\",\"+ str(node_label))\n",
    "        context_pair2 = (str(node_label) + \",\" + str(labelOfLastNode))\n",
    "        storeContextPairs(context_pair1, budgetOfLastNode, context_pairs)\n",
    "        storeContextPairs(context_pair2, budgetOfLastNode, context_pairs)\n",
    "        index_count = index_count + 1\n",
    "        \n",
    "        updateSetCount(startvertex, context_pair1, budgetOfLastNode)\n",
    "        updateSetCount(startvertex, context_pair2, budgetOfLastNode)\n",
    "\n",
    "\n",
    "def addNewNodeToWindow(tempwindow, temp_window_count, window_size, vertex, budget):\n",
    "    newWindowElement = np.array([[vertex,budget]])\n",
    "    if temp_window_count+1 == window_size:\n",
    "        tempwindow = tempwindow[1:]\n",
    "        tempwindow[temp_window_count] = newWindowElement\n",
    "        return tempwindow, temp_window_count\n",
    "\n",
    "    tempwindow[temp_window_count+1] = newWindowElement\n",
    "    temp_window_count+=1\n",
    "    return tempwindow, temp_window_count\n",
    "\n",
    "def BFSRandomWalkWindow(startvertex, queue, queue_len, queue_pop_idx, queue_add_idx, context_pairs, window_size, walk_lenght):\n",
    "    queue_buffer_size = (queue.size/5)-1\n",
    "    while queue_len > 0:\n",
    "        vertex, budget, current_walk_lenght, window_count, window = queue[queue_pop_idx] \n",
    "\n",
    "        if all_sets[vertex] and vertex != startvertex:\n",
    "            current_amount_of_cp = len(all_sets[startvertex])\n",
    "            needed_amount_cp = len(all_sets[1])\n",
    "            cp_to_sample = needed_amount_cp - current_amount_of_cp\n",
    "            sampled_cp = random.sample(all_sets[vertex], cp_to_sample)\n",
    "            for cp in sampled_cp:\n",
    "                storeContextPairs(cp, 1, context_pairs) \n",
    "            all_sets[startvertex].extend(sampled_cp)\n",
    "            break\n",
    "        \n",
    "        queue_len -=1\n",
    "        if queue_len > 0:\n",
    "            if queue_pop_idx == queue_buffer_size:\n",
    "                queue_pop_idx = 0\n",
    "            else:\n",
    "                queue_pop_idx += 1\n",
    "        else:\n",
    "            queue_add_idx = 0  \n",
    "    \n",
    "        vertex_neighbors = adjdict[vertex]\n",
    "        num_neighbors = vertex_neighbors.size\n",
    "        per_node_budget = getPerNodeBudget(num_neighbors, budget)\n",
    "        remainder = budget - (per_node_budget * num_neighbors)\n",
    "        \n",
    "        bonus = 0\n",
    "        if remainder > 0:\n",
    "            np.random.shuffle(vertex_neighbors) \n",
    "            bonus = remainder \n",
    "            \n",
    "        current_walk_lenght += 1\n",
    "        for neighbor in vertex_neighbors:\n",
    "    \n",
    "            budget_for_this_node = per_node_budget \n",
    "            temp_window = np.copy(window)\n",
    "            temp_window_count = window_count \n",
    "            \n",
    "            if bonus == 0 and budget_for_this_node == 0:\n",
    "                break\n",
    "             \n",
    "            if bonus > 0:    \n",
    "                bonus -= 1\n",
    "                budget_for_this_node += 1          \n",
    "                     \n",
    "            temp_window, temp_window_count = addNewNodeToWindow(temp_window, temp_window_count, window_size, neighbor, budget_for_this_node)\n",
    "            updateContextPairs(startvertex, temp_window, temp_window_count, context_pairs) \n",
    "            if current_walk_lenght < walk_lenght:\n",
    "                newQueueElement = np.array([[neighbor, budget_for_this_node, current_walk_lenght, temp_window_count, temp_window]])\n",
    "                queue[queue_add_idx] = newQueueElement\n",
    "                queue_len+=1\n",
    "                if queue_add_idx == queue_buffer_size:\n",
    "                    queue_add_idx = 0\n",
    "                    continue\n",
    "                queue_add_idx += 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Runner():\n",
    "    start = time.time()\n",
    "    context_pairs = {}    \n",
    "    print(\"Running BFSRandomWalkWindow...\")\n",
    "\n",
    "    for startvertex in adjdict.keys():\n",
    "        \n",
    "        #print(\"\")\n",
    "        #print(\"Running from -> \", startvertex, \"with budget ->\", BUDGET)\n",
    "        \n",
    "        window_count = 0\n",
    "        WINDOW = np.zeros(shape=(WINDOW_SIZE+20,2),dtype=int)\n",
    "        firstWindowElement = np.array([[startvertex, BUDGET]])\n",
    "        WINDOW[window_count] = firstWindowElement\n",
    "\n",
    "        queue_len = 0\n",
    "        queue_pop_idx = 0\n",
    "        queue_add_idx = 0\n",
    "        queue = np.zeros(shape=(BUDGET,5),dtype=object)\n",
    "        firstQueueElement = np.array([[startvertex, BUDGET, 1, window_count, WINDOW]])\n",
    "        queue[queue_len] = firstQueueElement\n",
    "        queue_len+=1\n",
    "        queue_add_idx+=1\n",
    "\n",
    "        BFSRandomWalkWindow(startvertex, queue, queue_len, queue_pop_idx, queue_add_idx, context_pairs, WINDOW_SIZE, WALK_LENGHT)\n",
    "    \n",
    "    end = time.time()\n",
    "    result = end - start\n",
    "    print(\"Run in :\",result)\n",
    "    return context_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFSRandomWalkWindow...\n",
      "Run in : 1607.4035108089447\n"
     ]
    }
   ],
   "source": [
    "contextPairs = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value sums up to:  569222400\n"
     ]
    }
   ],
   "source": [
    "# Count the total count sum\n",
    "CountContextPairs(contextPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing context pairs to file...\")    \n",
    "#Writing to file    \n",
    "for (key, value) in contextPairs.items():\n",
    "    femb_iterative.write(str(key) + \" \" + str(value) + \"\\n\" )\n",
    "femb_iterative.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeContextPairs(context_pair, budget, context_pairs):\n",
    "    if context_pair not in context_pairs:\n",
    "        context_pairs[context_pair] = budget\n",
    "    else:\n",
    "        context_pairs[context_pair] = context_pairs[context_pair] + budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = [{\"a\":4, \"b\":5},{\"a\":1, \"b\":3, \"f\":7},{\"a\":4, \"b\":5, \"c\":4}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 9, 'b': 13, 'f': 7, 'c': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CountDictListCp(lis):\n",
    "    contextPairs = {}\n",
    "    for dicset in lis:\n",
    "        for CP,budget in dicset.items():\n",
    "            storeContextPairs(CP,budget,contextPairs)\n",
    "    return contextPairs\n",
    "        \n",
    "cp = CountDictListCp(lis)\n",
    "cp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import shared_memory\n",
    "shm_a = shared_memory.SharedMemory(create=True, size=10)\n",
    "type(shm_a.buf)\n",
    "<class 'memoryview'>\n",
    "buffer = shm_a.buf\n",
    "len(buffer)\n",
    "10\n",
    "buffer[:4] = bytearray([22, 33, 44, 55])  # Modify multiple at once\n",
    "buffer[4] = 100                           # Modify single byte at a time\n",
    "# Attach to an existing shared memory block\n",
    "shm_b = shared_memory.SharedMemory(shm_a.name)\n",
    "import array\n",
    "array.array('b', shm_b.buf[:5])  # Copy the data into a new array.array\n",
    "array('b', [22, 33, 44, 55, 100])\n",
    "shm_b.buf[:5] = b'howdy'  # Modify via shm_b using bytes\n",
    "bytes(shm_a.buf[:5])      # Access via shm_a\n",
    "b'howdy'\n",
    "shm_b.close()   # Close each SharedMemory instance\n",
    "shm_a.close()\n",
    "shm_a.unlink()  # Call unlink only once to release the shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'shared_memory' from 'multiprocessing' (C:\\Users\\Max\\Anaconda3\\lib\\multiprocessing\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-89388ae3e16c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshared_memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mshm_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshared_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSharedMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshm_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'shared_memory' from 'multiprocessing' (C:\\Users\\Max\\Anaconda3\\lib\\multiprocessing\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from multiprocessing import shared_memory\n",
    "shm_a = shared_memory.SharedMemory(create=True, size=10)\n",
    "type(shm_a.buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([1,2,3,3,3,3], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

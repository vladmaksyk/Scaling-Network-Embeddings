{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import math \n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import array as arr\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import tqdm\n",
    "import AmortizedRandomWalk\n",
    "\n",
    "filepathTXT = \"../edgelists/BlogCatalog-edgelist.txt\"\n",
    "filepathCSV = \"../edgelists/BlogCatalog-edgelist.csv\"\n",
    "embeddingsrecursive = \"../embeddings/BlogCatalog-edgelist.txt.embeddings-recursive\"\n",
    "embeddingsiterative = \"../embeddings/BlogCatalog-edgelist.txt.embeddings-iterative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseEdgeList2(graph_file, direction=\"undirected\"):\n",
    "    # Create Graph\n",
    "    G = nx.Graph()\n",
    "    # Create head\n",
    "    colNames=[\"Start\", \"End\"]\n",
    "    edgeData = pd.read_csv(filepathCSV, names=colNames)\n",
    "\n",
    "    #Add nodes\n",
    "    nodes = []\n",
    "    #loop throug data records\n",
    "    for i in range (0, edgeData.shape[0]):\n",
    "        #append every node\n",
    "        nodes.append(edgeData.iloc[i,0])\n",
    "        nodes.append(edgeData.iloc[i,1])\n",
    "    #creating a set of nodes    \n",
    "    nodes = set(nodes)\n",
    "    #sorting the nodes in increasing order\n",
    "    uniqueNodes = (list(nodes))\n",
    "    uniqueNodes.sort()\n",
    "    #adding the nodes to the graph\n",
    "    G.add_nodes_from(uniqueNodes)\n",
    "\n",
    "    # Add edges\n",
    "    #loop from 0 to amount of records\n",
    "    edgeCount = 0\n",
    "    for i in range (0, edgeData.shape[0]):\n",
    "        edgeCount += 1\n",
    "        #add the edge to the graph\n",
    "        G.add_edge(edgeData.iloc[i,0], edgeData.iloc[i,1])\n",
    "    print(\"Nodes: \", G.number_of_nodes(),\" Edges: \", G.number_of_edges(), \" loaded from \", graph_file)\n",
    "\n",
    "    if(direction == \"undirected\"):\n",
    "        return G.to_undirected()\n",
    "    else:\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdjNPList(graph):\n",
    "    adjdict = {}\n",
    "    for vertex in graph:\n",
    "        adjdict[vertex] = np.array([n for n in G.neighbors(vertex)]) \n",
    "        np.random.shuffle(adjdict[vertex])\n",
    "    return adjdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "femb_recursive = open(embeddingsrecursive, 'w')\n",
    "femb_iterative = open(embeddingsiterative, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toyGraph():\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from([1,2,3,4,5,6,7,8,9,10,11])\n",
    "    G.add_edge(2, 1);G.add_edge(3, 1);G.add_edge(3, 2);G.add_edge(4, 1);\n",
    "    G.add_edge(4, 2);G.add_edge(4, 3);G.add_edge(5, 1);G.add_edge(6, 1);\n",
    "    G.add_edge(7, 1);G.add_edge(7, 5);G.add_edge(7, 6);G.add_edge(8, 1);\n",
    "    G.add_edge(8, 2);G.add_edge(8, 3);G.add_edge(8, 4);G.add_edge(9, 1); \n",
    "    G.add_edge(9, 3);G.add_edge(10, 3);G.add_edge(11, 1);G.add_edge(11, 5);\n",
    "    print(\"Nodes: \", G.number_of_nodes(),\" Edges: \", G.number_of_edges())\n",
    "    # Draw graph\n",
    "    # nx.draw(G, with_labels = True)\n",
    "    # plt.show()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseNodes(list_nodes, sample_size):\n",
    "    return random.sample(population=list_nodes, k=sample_size) # 2.22 s Â± 179 ms per loop  (labesl str)\n",
    "\n",
    "def getPerNodeBudget(numNodes, budget):\n",
    "    return math.floor(budget/numNodes)\n",
    "\n",
    "def storeContextPairs(context_pair, budget, context_pairs):\n",
    "    if context_pair not in context_pairs:\n",
    "        context_pairs[context_pair] = budget\n",
    "    else:\n",
    "        context_pairs[context_pair] = context_pairs[context_pair] +  budget\n",
    "        \n",
    "def updateContextPairs(window, window_count, context_pairs):\n",
    "    lastNode = window[window_count]\n",
    "    labelOfLastNode, budgetOfLastNode = lastNode\n",
    "    index_count = 0\n",
    "    window_except_last = window[:window_count]\n",
    "    window_reversed = window_except_last[::-1]\n",
    "    for node in window_reversed:\n",
    "        node_label = node[0]\n",
    "        if index_count == ORIGINAL_WINDOW_SIZE:\n",
    "            break\n",
    "        context_pair1 = (labelOfLastNode, node_label)\n",
    "        context_pairs.append(context_pair1)\n",
    "        \n",
    "        context_pair2 = (node_label, labelOfLastNode)\n",
    "        context_pairs.append(context_pair2)\n",
    "        \n",
    "        index_count = index_count + 1\n",
    "    return context_pairs\n",
    "\n",
    "def addNewNodeToWindow(tempwindow, temp_window_count, window_size, vertex, budget):\n",
    "    newWindowElement = np.array([[vertex,budget]])\n",
    "    if temp_window_count+1 == window_size:\n",
    "        tempwindow = tempwindow[1:]\n",
    "        tempwindow[temp_window_count] = newWindowElement\n",
    "        return tempwindow, temp_window_count\n",
    "\n",
    "    tempwindow[temp_window_count+1] = newWindowElement\n",
    "    temp_window_count+=1\n",
    "    return tempwindow, temp_window_count\n",
    "\n",
    "def BFSRandomWalkWindow(startvertex):\n",
    "    context_pairs = []\n",
    "    \n",
    "    window_count = 0\n",
    "    WINDOW = np.zeros(shape=(WINDOW_SIZE+20,2),dtype=int)\n",
    "    firstWindowElement = np.array([[startvertex, BUDGET]])\n",
    "    WINDOW[window_count] = firstWindowElement\n",
    "\n",
    "    queue_len = 0\n",
    "    queue_pop_idx = 0\n",
    "    queue_add_idx = 0\n",
    "    queue = np.zeros(shape=(BUDGET,5),dtype=object)\n",
    "    firstQueueElement = np.array([[startvertex, BUDGET, 1, window_count, WINDOW]])\n",
    "    queue[queue_len] = firstQueueElement\n",
    "    queue_len+=1\n",
    "    \n",
    "    queue_buffer_size = (queue.size/5)-1\n",
    "    \n",
    "    while queue_len > 0:\n",
    "        vertex, budget, current_walk_lenght, window_count, window = queue[queue_pop_idx] \n",
    "        queue_len -=1\n",
    "        if queue_len > 0:\n",
    "            if queue_pop_idx == queue_buffer_size:\n",
    "                queue_pop_idx = 0\n",
    "            else:\n",
    "                queue_pop_idx += 1\n",
    "        vertex_neighbors = adjdict[vertex]\n",
    "        num_neighbors = vertex_neighbors.size\n",
    "        m = getPerNodeBudget(num_neighbors, budget)\n",
    "        remainder = budget - (m * num_neighbors)\n",
    "        if remainder > 0:\n",
    "            np.random.shuffle(vertex_neighbors) \n",
    "        else:\n",
    "            remainder = num_neighbors \n",
    "        current_walk_lenght += 1\n",
    "        for neighbor in vertex_neighbors[:remainder]:\n",
    "            budget_for_this_node = m \n",
    "            temp_window = np.copy(window)\n",
    "            temp_window_count = window_count\n",
    "            if remainder != num_neighbors:\n",
    "                budget_for_this_node = budget_for_this_node + 1\n",
    "            temp_window, temp_window_count = addNewNodeToWindow(temp_window, temp_window_count, WINDOW_SIZE, neighbor, budget_for_this_node) \n",
    "            context_pairs = updateContextPairs(temp_window, temp_window_count, context_pairs) \n",
    "            if current_walk_lenght < WALK_LENGHT:\n",
    "                newQueueElement = np.array([[neighbor, budget_for_this_node, current_walk_lenght, temp_window_count, temp_window]])\n",
    "                queue[queue_add_idx] = newQueueElement\n",
    "                if queue_add_idx == queue_buffer_size:\n",
    "                    queue_add_idx = 0\n",
    "                else:\n",
    "                    queue_add_idx += 1\n",
    "                queue_len+=1\n",
    "    return context_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set toy parameters and graph\n",
    "# WALK_LENGHT = 3\n",
    "# BUDGET = 1\n",
    "# ORIGINAL_WINDOW_SIZE = 1\n",
    "# WINDOW_SIZE = ORIGINAL_WINDOW_SIZE*2+1\n",
    "\n",
    "G = toyGraph()\n",
    "adjdict = getAdjNPList(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the actual parameters and graph\n",
    "# WALK_LENGHT = 40\n",
    "# BUDGET = 80\n",
    "# ORIGINAL_WINDOW_SIZE = 10\n",
    "# WINDOW_SIZE = ORIGINAL_WINDOW_SIZE*2+1\n",
    "\n",
    "G = parseEdgeList2(filepathCSV) # Integer labels\n",
    "adjdict = getAdjNPList(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Runner():   \n",
    "    all_context_pairs = []\n",
    "    for startvertex in adjdict.keys(): \n",
    "        context_pairs = BFSRandomWalkWindow(startvertex)\n",
    "        all_context_pairs.extend(context_pairs)\n",
    "    return all_context_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextPairs = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total count sum\n",
    "countSum = 0\n",
    "for key, value in contextPairs.items():\n",
    "    countSum += value\n",
    "print(\"Total value sums up to: \", countSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total count sum\n",
    "print(len(contextPairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextPairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing context pairs to file...\")    \n",
    "#Writing to file    \n",
    "for (key, value) in contextPairs.items():\n",
    "    femb_iterative.write(str(key) + \" \" + str(value) + \"\\n\" )\n",
    "femb_iterative.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "import worker\n",
    "if __name__ ==  '__main__': \n",
    "    num_processors = 3\n",
    "    a = 5\n",
    "    p = Pool(processes = num_processors)\n",
    "    data = [(1,2),(3,4),(5,6),(7,8)]\n",
    "    out1 = p.map(worker.worker,([i for i in data]))\n",
    "    print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ ==  '__main__': \n",
    "    start = time.time()\n",
    "    num_processors = 4\n",
    "    p = Pool(processes = num_processors)\n",
    "    nodes = [i for i in adjdict.keys()]\n",
    "    contextPairs = p.starmap(AmortizedRandomWalk.BFSRandomWalkWindow, zip(nodes, repeat(adjdict)))\n",
    "    end = time.time()\n",
    "    result = end - start\n",
    "    print(\"The execution time ->\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contextPairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "for cp_set in contextPairs:\n",
    "    total_count += len(cp_set)\n",
    "print(total_count)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
